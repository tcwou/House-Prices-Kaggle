{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Goal\n",
    "\n",
    "The goal of this project is to predict housing prices from the ['Ames Housing Dataset'](http://www.amstat.org/publications/jse/v19n3/decock.pdf), involving 79 explanatory variables including continuous, categorical and ordinal data types.\n",
    "\n",
    "The dataset is part of the ['Housing Prices: Advanced Regression Techniques'](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition hosted by Kaggle. Scores are assessed via RMSLE, and scores ranked on the leaderboard.\n",
    "\n",
    "The RMLSE achieved by the top performing Elastic Net model is 0.11501, placing in the top 7% of scores. \n",
    "\n",
    "This notebook will discuss the results and decision-making for the initial data exploration, missing value imputation, feature engineering, feature selection and regression modelling. \n",
    "\n",
    "The models used to fit the data include Ridge Regression, ElasticNet Regression and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Exploration\n",
    "\n",
    "Import the modules we will be using for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'XGBRegressor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f756a4f00617>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mElasticNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_ridge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKernelRidge\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'XGBRegressor'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None # remove dataframe copy warning messages\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial data exploration reveals there are 36 numeric type features, and 43 non-numeric features.\n",
    "The total number of rows is 1460, however the counts for many columns is less than 1460, indicating missing values.\n",
    "Some features such as 'PoolQC' and 'MiscFeature' have a majority of missing values.\n",
    "\n",
    "'SalePrice' is the target variable we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(data.describe(include='all'))\n",
    "print('Total data dimensions', data.shape)\n",
    "print('Data dimensions of numeric features', data.select_dtypes(include=[np.number]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Id' column is a unique identifier for each row of data, and does not have any predictive power. We can safely remove these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test['Id'] # Save to create .csv submission file\n",
    "data = data.drop(columns=['Id'])\n",
    "test = test.drop(columns=['Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check for features with only one unique value, indicating the feature will have no predictive power. As there is no output, all features have more than one unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data:\n",
    "    if len(data[column].unique()) <= 1:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Correlation\n",
    "\n",
    "Pearson correlations between numerical variables can be visualized using a heatmap. Zero represents no correlation. +1.0 indicates perfect positive correlation, 0 represents no correlation, and -1.0 indicates perfect negative correlation.\n",
    "\n",
    "The heatmap reveals the most correlated numerical features with 'SalePrice' are 'OverallQual' - The overall quality of the property on an ordinal 1-10 scale, and 'GrLivArea' the area of the living spaces above ground.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "correlations = numeric_data[['GarageCars', 'GarageYrBlt', 'TotRmsAbvGrd', 'SalePrice']]\n",
    "heatmap_correlations = correlations\n",
    "\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(heatmap_correlations, annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at correlations between variables we see that several have corelation values > 0.8. Namely, ('GarageCars' and 'GarageArea'), ('GarageYrBlt' and 'YearBuilt'), and ('GrLivArea' and 'TotRmsAbvGrd'). The presence of collinearity can inflate the variance and cause instability in coefficient estimates, so we will remove one variable from each pair of collinear variables. \n",
    "\n",
    "Intuitively, the variables represent the same information, for example, the number of cars and the area of the garage are both giving the same information - the size of the garage. Therefore it is probably safe to remove one of these variables for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['GarageCars', 'GarageYrBlt', 'TotRmsAbvGrd'])\n",
    "test = test.drop(columns=['GarageCars', 'GarageYrBlt', 'TotRmsAbvGrd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "\n",
    "By investigating the relationship of each feature with 'SalePrice' we can gain an idea of the distribution of the data, and the presence of any outliers. \n",
    "\n",
    "There appear to be several points, for example in the 'LotFrontage', and 'LotArea' feature plots, that lie very far from the main group of datapoints. We will plot these features in more detail in the next cell and explore these points further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "print(numeric_data.columns)\n",
    "\n",
    "%matplotlib inline\n",
    "f, ax = plt.subplots(6, 6, figsize=(22,11))\n",
    "for row in range(6):\n",
    "    for col in range(6):\n",
    "        idx = col+(6*row)\n",
    "        if idx < 34:\n",
    "            x = numeric_data.iloc[:, idx]\n",
    "            y = numeric_data['SalePrice']\n",
    "            ax[row, col].scatter(x, y, s=5)\n",
    "            ax[row, col].set_title(numeric_data.columns[idx])\n",
    "            ax[row, col].set_yticklabels([])\n",
    "f.delaxes(ax[5, 4])\n",
    "f.delaxes(ax[5, 5])\n",
    "plt.subplots_adjust(hspace=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify possible outliers we can calculate the z-score for each feature. In general, a z-score of 3 or more is indicative of an outlier. Using this constraint on any feature yields many dozens of potential outliers. To constrain our outlier search we choose to identify outliers with z-score >= 5. To avoid information overload, we will look at just 6 continuous features that have high correlation with 'SalePrice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_features = ['LotFrontage', 'LotArea', 'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea']\n",
    "outlier_data = data.copy()\n",
    "\n",
    "zscore_cols = []\n",
    "# Calculate Z-score for each feature of interest\n",
    "for col in outlier_features:\n",
    "    col_zscore = col + '_zscore'\n",
    "    zscore_cols += [col_zscore]\n",
    "    outlier_data[col_zscore] = (outlier_data[col] - outlier_data[col].mean())/outlier_data[col].std(ddof=0)\n",
    "\n",
    "# Get index values of points with z-score >= 5 for any feature\n",
    "outlier_data = outlier_data[zscore_cols]\n",
    "outliers = outlier_data[(outlier_data >= 5).any(axis=1)]\n",
    "outliers_list = outliers.index.tolist()\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# Add SalePrice for plotting convenience \n",
    "outlier_features += ['SalePrice']\n",
    "data_only_outliers = data.iloc[outliers_list][outlier_features]\n",
    "data_outliers = data[outlier_features]\n",
    "\n",
    "f, ax = plt.subplots(2, 3, figsize=(14,7))\n",
    "for row in range(3):\n",
    "    for col in range(3):\n",
    "        idx = col+(3*row)\n",
    "        if idx < 6:\n",
    "            x = data_outliers.iloc[:, idx]\n",
    "            x_outlier = data_only_outliers.iloc[:, idx]\n",
    "            y = data_outliers['SalePrice']\n",
    "            y_outlier = data_only_outliers['SalePrice']\n",
    "            ax[row, col].scatter(x, y, s=5, c='b')\n",
    "            ax[row, col].scatter(x_outlier, y_outlier, s=7, c='r')\n",
    "            ax[row, col].set_title(data_outliers.columns[idx])\n",
    "            ax[row, col].set_yticklabels([])\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plots show that an outlier for one particular feature may not necessarily be an outlier in another. Upon further investigation of the ['Ames Housing Dataset'](http://www.amstat.org/publications/jse/v19n3/decock.pdf) the author of the dataset reveals that there are three true outliers that involve partial sales. He suggests to drop datapoints with 'GrLivArea' > 4000.\n",
    "\n",
    "After applying the suggested filter, we observe a tighter distribution in the six features, with the number of outliers decreasing from 11 to 8. While the remaining 8 points have a high z-score, without further information there is no reason to believe these are outliers, so we will keep these points in our dataset for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter ['GrLivArea'] < 4000\n",
    "data = data[data['GrLivArea']<4000]\n",
    "\n",
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Re-plot outlier graphs\n",
    "data_only_outliers = data.iloc[outliers_list][outlier_features]\n",
    "data_outliers = data[outlier_features]\n",
    "\n",
    "f, ax = plt.subplots(2, 3, figsize=(14,7))\n",
    "for row in range(3):\n",
    "    for col in range(3):\n",
    "        idx = col+(3*row)\n",
    "        if idx < 6:\n",
    "            x = data_outliers.iloc[:, idx]\n",
    "            x_outlier = data_only_outliers.iloc[:, idx]\n",
    "            y = data_outliers['SalePrice']\n",
    "            y_outlier = data_only_outliers['SalePrice']\n",
    "            ax[row, col].scatter(x, y, s=5, c='b')\n",
    "            ax[row, col].scatter(x_outlier, y_outlier, s=7, c='r') # outliers will appear red\n",
    "            ax[row, col].set_title(data_outliers.columns[idx])\n",
    "            ax[row, col].set_yticklabels([])\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "\n",
    "Missing values must be imputed in order to retain the maximum information for model training.\n",
    "Initial exploration of the data reveals missing data in both the train and test sets. Values will be imputed according to data type and number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Set Missing Values')\n",
    "print()\n",
    "for col in data:\n",
    "    num_missing = len(test) - data[col].count()\n",
    "    if num_missing > 0:\n",
    "        print(col, num_missing, str(round(num_missing/len(data)*100))+ '%')\n",
    "print()\n",
    "print('Test Set Missing Values')\n",
    "print()\n",
    "for col in test:\n",
    "    num_missing = len(test) - test[col].count()\n",
    "    if num_missing > 0:\n",
    "        print(col, num_missing, str(round(num_missing/len(test)*100))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspection of the data, for most categorical features a missing value indicates that the feature is not present, rather than missing data. For example, 'PoolQC'= NA means the house does not have a pool, rather than there is missing data on the pool quality. For these data points we will replace NA with 'NotPresent'.\n",
    "\n",
    "The remaining missing values in the categorical variables will be imputed with the mode value.\n",
    "\n",
    "The missing values for continuous garage and basement values correspond to the non-presence of a garage or basement, therefore these values should be imputed as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA means feature not prsent. Fill NA with not present\n",
    "for feature in ['Alley', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 'BsmtCond', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']:\n",
    "    data[feature] = data[feature].fillna('NotPresent')\n",
    "    test[feature] = test[feature].fillna('NotPresent')\n",
    "    \n",
    "#For remaining categorical data impute with mode value\n",
    "for feature in ['MasVnrType', 'Electrical', 'MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'KitchenQual', 'Functional', 'SaleType']:\n",
    "    value = data[feature].value_counts().keys()[0] # The key of value_counts() at index[0] is the mode value\n",
    "    data[feature] = data[feature].fillna(value)\n",
    "    test[feature] = test[feature].fillna(value)\n",
    "    \n",
    "#For MasVnrArea since the missing rows have MasVnrType as None, the Area should be zero\n",
    "data['MasVnrArea'] = data['MasVnrArea'].fillna(0)\n",
    "test['MasVnrArea'] = test['MasVnrArea'].fillna(0)\n",
    "\n",
    "\n",
    "#For BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath, BsmtHalfBath if there is no basment, these missing values should be zero\n",
    "for feature in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n",
    "    data.loc[data['BsmtFinType1'] == 'NotPresent', feature] = 0\n",
    "    test.loc[test['BsmtFinType1'] == 'NotPresent', feature] = 0\n",
    "    \n",
    "#For GaragaArea if there is no garage the missing value should be zero\n",
    "data.loc[data['GarageFinish'] == 'NotPresent', 'GarageArea'] = 0\n",
    "test.loc[test['GarageFinish'] == 'NotPresent', 'GarageArea'] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of missing values in 'LotFrontage' is 18%. This is much higher than for other continuous variables. 'LotFrontage' represents the linear feet of street connected to the property. \n",
    "\n",
    "From the correlation heatmap, the most correlated variables to 'LotFrontage' are 'LotArea' and '1stFlrSF'. Therefore we can use these two features and make a simple linear regression model to impute the values of 'LotFrontage' more accurately than using the a single descriptive statistic.\n",
    "\n",
    "The residual plot shows two outliers, however the majority of points lie within the main cluster. Ignoring the outliers the cluster is fairly random, so this model is probably good enough to impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotdata = data[['LotArea', '1stFlrSF', 'LotFrontage']]\n",
    "lotdata = lotdata.dropna()\n",
    "\n",
    "x = lotdata[['LotArea', '1stFlrSF']]\n",
    "y = lotdata['LotFrontage']\n",
    "\n",
    "y = y.values.reshape(-1, 1)\n",
    "all_x = data[['LotArea', '1stFlrSF']]\n",
    "test_all_x = test[['LotArea', '1stFlrSF']]\n",
    "\n",
    "linear_lotfrontage = LinearRegression()\n",
    "linear_lotfrontage.fit(x, y)\n",
    "\n",
    "linear_pred = linear_lotfrontage.predict(all_x)\n",
    "test_linear_pred = linear_lotfrontage.predict(test_all_x)\n",
    "\n",
    "# Create a new column for predicted values\n",
    "data['LotFrontagePred'] = linear_pred\n",
    "test['LotFrontagePred'] = test_linear_pred\n",
    "\n",
    "residuals_linear = data['LotFrontagePred'] - data['LotFrontage']\n",
    "\n",
    "# Plot residuals\n",
    "plt.scatter(data['LotFrontagePred'], residuals_linear)\n",
    "plt.title('LotFrontagePred Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used predicted values column to fill the missing data\n",
    "data['LotFrontage'] = data['LotFrontage'].fillna(data['LotFrontagePred'])\n",
    "test['LotFrontage'] = test['LotFrontage'].fillna(test['LotFrontagePred'])\n",
    "\n",
    "# Drop predicted value columns\n",
    "data = data.drop(columns=['LotFrontagePred'])\n",
    "test = test.drop(columns=['LotFrontagePred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Domain knowledge is key to identifying and creating pertinent features in order to build an accurate model. Insights from housing valuation websites such as [Opendoor](https://www.opendoor.com/w/blog/factors-that-influence-home-value) give an insight from industry experts into what should be included in the model.\n",
    "\n",
    "The eight most important factors from this blog include past prices, location, home size, age, condition and upgrades. While many of these factors are captured in the provided features, in order to provide the best model we will modify existing features and create new ones as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total bathrooms of the property may be calculated by the addition of 'BsmtFullBath', 'FullBath', 'BsmtHalfBath' and 'HalfBath' features.\n",
    "\n",
    "Likewise, the total livable square footage of the property may be calulcated by the addition of 'TotalBsmtSF' and 'GrLivArea'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TotalBaths'] = data['BsmtFullBath'] + data['FullBath'] + 0.5*(data['BsmtHalfBath'] + data['HalfBath'])\n",
    "test['TotalBaths'] = test['BsmtFullBath'] + test['FullBath'] + 0.5*(test['BsmtHalfBath'] + test['HalfBath'])\n",
    "\n",
    "data['TotalSF'] = data['TotalBsmtSF'] + data['GrLivArea']\n",
    "test['TotalSF'] = test['TotalBsmtSF'] + test['GrLivArea']\n",
    "\n",
    "data = data.drop(columns=['BsmtFullBath', 'FullBath', 'BsmtHalfBath', 'HalfBath'])\n",
    "test = test.drop(columns=['BsmtFullBath', 'FullBath', 'BsmtHalfBath', 'HalfBath'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'MoSold' is a numeric data type column represents the month each property was sold. This column should be converted into categories. With investigation of the column it appears that the average 'SalePrice' depends on the time of the year. To capture this we can group the months into different seasons, represented by Q1, Q2, Q3 and Q4.\n",
    "\n",
    "The 'MSSubClass' is another numeric data type column where each value represents a categorical code, so this column will also be converted into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Categorize MoSold\n",
    "data = data.replace({'MoSold': {1: 'Q1', 2: 'Q1',3: 'Q1',\n",
    "4: 'Q2',5: 'Q2',6: 'Q2',7: 'Q3',8: 'Q3',9: 'Q3',10: 'Q4',\n",
    "11: 'Q4',12: 'Q4'}})\n",
    "\n",
    "test = test.replace({'MoSold': {1: 'Q1', 2: 'Q1',3: 'Q1',\n",
    "4: 'Q2',5: 'Q2',6: 'Q2',7: 'Q3',8: 'Q3',9: 'Q3',10: 'Q4',\n",
    "11: 'Q4',12: 'Q4'}})\n",
    "\n",
    "print(data.groupby(['MoSold'])['SalePrice'].mean())\n",
    "\n",
    "# Categorize MSSubClass\n",
    "data = data.replace({'MSSubClass': {20: 'class20', 30: 'class30',40: 'class40',\n",
    "45: 'class45',50: 'class50',60: 'class60',70: 'class70',\n",
    "75: 'class75',80: 'class80',85: 'class85',90: 'class90',\n",
    "120: 'class120',150: 'class150',160: 'class160',180: 'class180',\n",
    "190: 'class190'}})\n",
    "\n",
    "test = test.replace({'MSSubClass': {20: 'class20', 30: 'class30',40: 'class40',\n",
    "45: 'class45',50: 'class50',60: 'class60',70: 'class70',\n",
    "75: 'class75',80: 'class80',85: 'class85',90: 'class90',\n",
    "120: 'class120',150: 'class150',160: 'class160',180: 'class180',\n",
    "190: 'class190'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age of the property is not currently included in the features, however we can calculate this from the 'YrSold' and the 'YearBuilt' columns.\n",
    "\n",
    "The 'YearRemodAdd' columns is the year of the most recent remodel or addition to the property. This column defaults to 'YearBuilt' if there was no remodelling or additions. As this column will be highly correlated with 'YearBuilt' we can create a new categorical variable indicating whether a remodelling or addition has been done.\n",
    "\n",
    "We see that houses that have not been remodelled have a higher average 'SalePrice' than those that have.\n",
    "\n",
    "The year sold should also be converted into a categorical column as the events of the period, namely, before and after the 2008 recession, have a strong effect on the SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create 'AgeSold' feature\n",
    "data['AgeSold'] = data['YrSold'] - data['YearBuilt']\n",
    "test['AgeSold'] = test['YrSold'] - test['YearBuilt']\n",
    "\n",
    "# Create 'Remodel' indicator feature\n",
    "data['Remodel'] = data['YearRemodAdd'] - data['YearBuilt']\n",
    "test['Remodel'] = test['YearRemodAdd'] - data['YearBuilt']\n",
    "\n",
    "data.loc[data['Remodel'] > 0, 'Remodel'] = 'Yes'\n",
    "data.loc[data['Remodel'] == 0, 'Remodel'] = 'No'\n",
    "test.loc[test['Remodel'] > 0, 'Remodel'] = 'Yes'\n",
    "test.loc[test['Remodel'] == 0, 'Remodel'] = 'No'\n",
    "\n",
    "data.loc[data['AgeSold'] == -1, 'AgeSold'] = 0\n",
    "test.loc[test['AgeSold'] == -1, 'AgeSold'] = 0\n",
    "\n",
    "print(data.groupby(['Remodel'])['SalePrice'].mean())\n",
    "print()\n",
    "\n",
    "# Convert 'YrSold' to object type\n",
    "data['YrSold'] = data.YrSold.astype(str)\n",
    "test['YrSold'] = test.YrSold.astype(str)\n",
    "\n",
    "print(data.groupby(['YrSold'])['SalePrice'].mean())\n",
    "\n",
    "\n",
    "# Remove redundant variables\n",
    "data = data.drop(columns=['YearBuilt', 'YearRemodAdd'])\n",
    "test = test.drop(columns=['YearBuilt', 'YearRemodAdd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, all features are linear and untransformed, however for highly correlated continuous features there may be a benefit to add quadratic and interactions terms. Referring back to the 'SalePrice' - Feature scatter plots in the outlier section, visually, the highly correlated variables may have some quadratic shape, therefore for Pearson correlation >0.3 we will add second power terms.\n",
    "\n",
    "Interaction terms will also give our model more predictive ability. We will address this after we finish preparing the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Create a dictionary where the key is the column, and the value is the correlation\n",
    "numeric_corr_dict = {}\n",
    "for col in numeric_data.columns:\n",
    "    numeric_corr_dict[col] = numeric_data[col].corr(data['SalePrice'])\n",
    "    \n",
    "# Sort the dictionary and select only values with correlation > 0.3\n",
    "numeric_corr_dict_sorted = sorted(numeric_corr_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "numeric_quadratics =  [k for k,v in numeric_corr_dict.items() if abs(v) >= 0.3]\n",
    "\n",
    "numeric_quadratics.remove('SalePrice')\n",
    "\n",
    "# Create power term features\n",
    "quadratic_columns = []\n",
    "for col in numeric_quadratics:\n",
    "    quadratic_columns += [col+\"**2\"]\n",
    "    data[col+\"**2\"] = data[col]**2\n",
    "    test[col+\"**2\"] = test[col] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the numerical features also seem to exhibit some skew and heteroscedasticity, however linear regression does not require that predictor variables are normally distributed, only that the errors (approximated by the residuals) should be normal. Decision tree based methods are also invariant to transformations in the variables.\n",
    "\n",
    "The response variable 'SalePrice' also exhibits right skew. While it is not a requirement that the response variable be normally distributed, there may be some benefit to the distribution of the errors. We can apply a log(1+p) transformation to the skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['SalePrice'])\n",
    "plt.title('SalePrice Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transforming the data we can see that the distribution of 'SalePrice' is no longer right skewed. We will also transform all features that exhibit skew greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Quantify skew for all variables\n",
    "skewed_table = abs(numeric_data.skew(axis = 0))\n",
    "\n",
    "# If skew is > 0.5, then apply the log1p transform\n",
    "skewed_numeric = skewed_table[skewed_table > 0.5].index\n",
    "skewed_numeric_test = skewed_numeric.drop('SalePrice')\n",
    "\n",
    "data[skewed_numeric] = np.log1p(data[skewed_numeric])\n",
    "test[skewed_numeric_test] = np.log1p(test[skewed_numeric_test])\n",
    "\n",
    "sns.distplot(data['SalePrice'])\n",
    "plt.title('SalePrice Distibution After Transform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots of categorical SalePrice - Categorical Feature show there are ordinal variables that have not been numerically coded. For all other categorical variables with no order we will dummy code each level into a separate feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data = data.select_dtypes(include=['object', 'category'])\n",
    "cat_data['SalePrice'] = data['SalePrice'].copy()\n",
    "\n",
    "f, ax = plt.subplots(7, 7, figsize=(20,10))\n",
    "for row in range(7):\n",
    "    for col in range(7):\n",
    "        idx = col+(7*row)\n",
    "        if idx < 44:\n",
    "            x=cat_data.iloc[:, idx]\n",
    "            y=cat_data['SalePrice']\n",
    "            ax[row, col].scatter(x, y, s=5)\n",
    "            ax[row, col].set_title(cat_data.columns[idx])\n",
    "            ax[row, col].set_yticklabels([])\n",
    "for i in range(6):\n",
    "    f.delaxes(ax[6, i+1])\n",
    "plt.subplots_adjust(hspace=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep only the ordinal variables that exhibit some correlation with 'SalePrice', taken to be > 0.3. The features that do not meet the threshold will be dummy coded along with the rest of the remaining categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List ordinal features according to their levels, ordinal_1 all share the same levels \n",
    "ordinal_1 = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "ordinal_2 = ['BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'GarageFinish', 'Fence']\n",
    "ordinal = ordinal_1 + ordinal_2\n",
    "\n",
    "# Create a function to save repeating code\n",
    "def ordinal_coding(df):\n",
    "    ord_data = df[ordinal]\n",
    "    for col in ordinal_1:\n",
    "        try:\n",
    "            ord_data.loc[ord_data[col] == 'Ex', col] = 5\n",
    "            ord_data.loc[ord_data[col] == 'Gd', col] = 4\n",
    "            ord_data.loc[ord_data[col] == 'TA', col] = 3\n",
    "            ord_data.loc[ord_data[col] == 'Fa', col] = 2\n",
    "            ord_data.loc[ord_data[col] == 'Po', col] = 1\n",
    "            ord_data.loc[ord_data[col] == 'NotPresent', col] = 0\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    ord_data.loc[ord_data['BsmtExposure'] == 'Gd','BsmtExposure'] = 4\n",
    "    ord_data.loc[ord_data['BsmtExposure'] == 'Av','BsmtExposure'] = 3\n",
    "    ord_data.loc[ord_data['BsmtExposure'] == 'Mn','BsmtExposure'] = 2\n",
    "    ord_data.loc[ord_data['BsmtExposure'] == 'No','BsmtExposure'] = 1\n",
    "    ord_data.loc[ord_data['BsmtExposure'] == 'NotPresent','BsmtExposure'] = 0\n",
    "\n",
    "    ord_data.loc[ord_data['BsmtFinType1'] == 'GLQ','BsmtFinType1'] = 6\n",
    "    ord_data.loc[ord_data['BsmtFinType1'] == 'ALQ','BsmtFinType1'] = 5\n",
    "    ord_data.loc[ord_data['BsmtFinType1'] == 'BLQ','BsmtFinType1'] = 4\n",
    "    ord_data.loc[ord_data['BsmtFinType1'] == 'Rec','BsmtFinType1'] = 3\n",
    "    ord_data.loc[ord_data['BsmtFinType1'] == 'LwQ','BsmtFinType1'] = 2\n",
    "    ord_data.loc[ord_data['BsmtFinType1'] == 'Unf','BsmtFinType1'] = 1\n",
    "    ord_data.loc[ord_data['BsmtFinType1'] == 'NotPresent','BsmtFinType1'] = 0\n",
    "\n",
    "    ord_data.loc[ord_data['BsmtFinType2'] == 'GLQ','BsmtFinType2'] = 6\n",
    "    ord_data.loc[ord_data['BsmtFinType2'] == 'ALQ','BsmtFinType2'] = 5\n",
    "    ord_data.loc[ord_data['BsmtFinType2'] == 'BLQ','BsmtFinType2'] = 4\n",
    "    ord_data.loc[ord_data['BsmtFinType2'] == 'Rec','BsmtFinType2'] = 3\n",
    "    ord_data.loc[ord_data['BsmtFinType2'] == 'LwQ','BsmtFinType2'] = 2\n",
    "    ord_data.loc[ord_data['BsmtFinType2'] == 'Unf','BsmtFinType2'] = 1\n",
    "    ord_data.loc[ord_data['BsmtFinType2'] == 'NotPresent','BsmtFinType2'] = 0\n",
    "\n",
    "    ord_data.loc[ord_data['Functional'] == 'Typ','Functional'] = 7\n",
    "    ord_data.loc[ord_data['Functional'] == 'Min1','Functional'] = 6\n",
    "    ord_data.loc[ord_data['Functional'] == 'Min2','Functional'] = 5\n",
    "    ord_data.loc[ord_data['Functional'] == 'Mod','Functional'] = 4\n",
    "    ord_data.loc[ord_data['Functional'] == 'Maj1','Functional'] = 3\n",
    "    ord_data.loc[ord_data['Functional'] == 'Maj2','Functional'] = 2\n",
    "    ord_data.loc[ord_data['Functional'] == 'Sev','Functional'] = 1\n",
    "    # ord_data.loc[ord_data['Functional'] == 'Sal','Functional'] = 0  # This level is missing from both train and test \n",
    "\n",
    "    ord_data.loc[ord_data['GarageFinish'] == 'Fin','GarageFinish'] = 3\n",
    "    ord_data.loc[ord_data['GarageFinish'] == 'RFn','GarageFinish'] = 2\n",
    "    ord_data.loc[ord_data['GarageFinish'] == 'Unf','GarageFinish'] = 1\n",
    "    ord_data.loc[ord_data['GarageFinish'] == 'NotPresent','GarageFinish'] = 0\n",
    "\n",
    "    ord_data.loc[ord_data['Fence'] == 'GdPrv','Fence'] = 4\n",
    "    ord_data.loc[ord_data['Fence'] == 'MnPrv','Fence'] = 3\n",
    "    ord_data.loc[ord_data['Fence'] == 'GdWo','Fence'] = 2\n",
    "    ord_data.loc[ord_data['Fence'] == 'MnWw','Fence'] = 1\n",
    "    ord_data.loc[ord_data['Fence'] == 'NotPresent','Fence'] = 0\n",
    "    return ord_data\n",
    "\n",
    "\n",
    "ord_data = ordinal_coding(data)\n",
    "test_ord_data = ordinal_coding(test)\n",
    "\n",
    "# Obtain the correlation for each ordinal variable, and keep only those with correlation > 0.3\n",
    "ord_data['SalePrice'] = data['SalePrice'].copy()\n",
    "ordinal_corr_dict = {}\n",
    "for col in ordinal:\n",
    "    ordinal_corr_dict[col] = ord_data[col].corr(ord_data['SalePrice'])\n",
    "    \n",
    "ordinal_keep =  [k for k,v in ordinal_corr_dict.items() if abs(v) >= 0.3]\n",
    "\n",
    "# Implement numeric coding of ordinal features\n",
    "data[ordinal_keep] = ord_data[ordinal_keep]\n",
    "test[ordinal_keep] = test_ord_data[ordinal_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the ordinal features have been converted to a numeric data type, we can start considering interactions between numeric features.\n",
    "\n",
    "Our data set has 37 numerical features, which means there will be 666 second order interaction terms added to our dataset. In order to prevent overfitting we must select only the most important interactions. Several machine learning models have implicit features selection such as lasso regression which utilizes L1 regularization to select features, and gradient boosting which can rank feature importance by calculating the improvement to the model if the feature is added.\n",
    "\n",
    "Feature selection was performed after all categorical variables were dummy coded, however the 15 highest lasso coefficients after normalization, and the 15 highest ranked gradient boosted features are seen below. The set of these interactions were used in the final model. The the number of interactions to include can be optimized to improve model performance even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric data without quadratic columns\n",
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "numeric_data = numeric_data.drop(columns=['SalePrice'])\n",
    "numeric_data = numeric_data.drop(columns=quadratic_columns)\n",
    "\n",
    "# Select only top 15 interactions from lasso and xgboost feature selection methods\n",
    "interactions_lasso = ['OverallQual:OverallCond', 'GrLivArea:TotalSF', 'LotArea:GrLivArea', 'OverallCond:ExterQual', 'ExterQual:Fireplaces', 'OverallCond:TotalSF', 'KitchenQual:TotalBaths', 'OverallQual:TotalBaths', '1stFlrSF:GrLivArea', 'KitchenQual:GarageCond', 'OverallQual:GarageQual', 'KitchenAbvGr:AgeSold', 'HeatingQC:TotalBaths', 'BsmtQual:TotalBaths', 'BsmtExposure:FireplaceQu']\n",
    "interactions_xgboost = ['OverallCond:TotalSF', 'OverallCond:AgeSold', 'ExterQual:AgeSold', 'LotArea:OverallCond', 'LotArea:GrLivArea', '1stFlrSF:AgeSold', 'OverallCond:TotalSF', 'LotFrontage:ScreenPorch', 'BsmtQual:OpenPorchSF', '1stFlrSF:GrLivArea', 'TotalBaths:AgeSold', 'OverallCond:GrLivArea', 'BsmtFinType1:BsmtFinSF1', 'HeatingQC:TotalSF', 'LotArea:TotalSF']\n",
    "interactions = list(set(interactions_lasso) | set(interactions_xgboost))\n",
    "\n",
    "# Add interaction terms\n",
    "for i in range(len(numeric_data.columns)):\n",
    "    i_col = numeric_data.columns[i]\n",
    "    for j in range(i + 1, len(numeric_data.columns)):\n",
    "        j_col = numeric_data.columns[j]\n",
    "        col_name = str(numeric_data.columns[i]) + ':' + numeric_data.columns[j]\n",
    "        data[col_name] = data[i_col] * data[j_col]\n",
    "        test[col_name] = test[i_col] * test[j_col]\n",
    "        if col_name not in interactions:\n",
    "            data = data.drop(columns=col_name)\n",
    "            test = test.drop(columns=col_name)\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now dummy code the remaining categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-ordinal categorical columns\n",
    "cat_cols = [col for col in cat_data.columns if col not in ordinal_keep]\n",
    "cat_cols.remove('SalePrice')\n",
    "\n",
    "dummy_df = pd.get_dummies(data[cat_cols])\n",
    "test_dummy_df = pd.get_dummies(test[cat_cols])\n",
    "\n",
    "data = pd.concat([data, dummy_df], axis=1)\n",
    "test = pd.concat([test, test_dummy_df], axis=1)\n",
    "\n",
    "data = data.drop(columns=cat_cols, axis=1)\n",
    "test = test.drop(columns=cat_cols, axis=1)\n",
    "\n",
    "# The train and test columns do not have the same levels so we create columns for missing levels to maintain dimensionality\n",
    "missing_cols = set(data.columns) - set(test.columns)\n",
    "for col in missing_cols:\n",
    "    test[col] = 0\n",
    "\n",
    "test = test[data.columns]\n",
    "test = test.drop(columns=['SalePrice'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we transformed many features in an effort to reduce skew. After further analysis many features still exhibit high levels of skew to the large number of zero values if the feature is not present.\n",
    "\n",
    "For example, consider 'MasVnrArea' - Masonry Veneer Area. The distribution of values is highly skewed since properties without this feature are set a value of 0. We can attempt to mitigate the effect of the zero-skew by adding additional dummy values that indicate whether the feature is zero or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['MasVnrArea'])\n",
    "plt.title('Zero-skew even after transform')\n",
    "plt.show()\n",
    "\n",
    "# Add dummy cols to reduce zero-skew in numeric columns \n",
    "zero_dummy_cols = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', '2ndFlrSF', 'LowQualFinSF', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'GarageArea**2', 'MasVnrArea**2', 'BsmtFinSF1**2','OpenPorchSF**2','TotalBsmtSF**2','WoodDeckSF**2']\n",
    "for col in zero_dummy_cols:\n",
    "    data['NonZero' + col] = (data[col] > 0).astype(int)\n",
    "    test['NonZero' + col] = (test[col] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After feature engineering we have a final count of 317 columns. As the goal of this project is to create a predictive model as opposed to an inferential model, we will go ahead and utilize all the features in models such as Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset has 317 columns\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression, Elastic net regression and gradient boosting are the three models that will be investigated. Hyperparameters are optimzied via grid search five fold cross valdidation. While the scoring method of the competition is RMSLE, negative RMSE will be used as a substitute that is available the sk-learn module.\n",
    "\n",
    "After fitting and elastic net model we see that the residual plot has many potential outliers in the plot. The best course of action would be to scrutinize the source of the data to determine if these data points are real or not. For now we will remove points with a residual greater than 0.35 (corresponding to a studentized residual greater than 3) and we will replot the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Elastic net model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "Y = data['SalePrice']\n",
    "X = data.drop(columns=['SalePrice'])\n",
    "\n",
    "\n",
    "en_model = ElasticNet(normalize=False)\n",
    "\n",
    "# To save computation time, optimized hyperparameters are provided\n",
    "\n",
    "\"\"\"\n",
    "params = {'alpha': np.linspace(0.0008, 0.0012, 3),\n",
    "          'l1_ratio': np.linspace(0.58, 0.66, 4)}\n",
    "\"\"\"\n",
    "\n",
    "params = {'alpha': [0.01], 'l1_ratio': [0.63]}\n",
    "\n",
    "grid = GridSearchCV(en_model, params, cv=5, scoring='neg_mean_squared_error')\n",
    "grid.fit(X, Y)\n",
    "best_params = grid.best_params_\n",
    "best_score = grid.best_score_\n",
    "best_knn = grid.best_estimator_\n",
    "print(best_score)\n",
    "en_coefs = pd.Series(grid.best_estimator_.coef_,index=X.columns)\n",
    "\n",
    "y_pred_en = grid.best_estimator_.predict(X)\n",
    "\n",
    "# Plot residuals\n",
    "residuals_train = y_pred_en - Y\n",
    "data['residual'] = residuals_train\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(y_pred_en, residuals_train)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the potential outliers we see that the residual plot is randomly distributed, and the negative RMSE has improved substantially from -0.01497 to -0.00827."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[abs(data['residual'])< 0.35]\n",
    "\n",
    "Y = data['SalePrice']\n",
    "X = data.drop(columns=['SalePrice', 'residual'])\n",
    "\n",
    "en_model = ElasticNet(normalize=False)\n",
    "\n",
    "\"\"\"\n",
    "params = {'alpha': np.linspace(0.0002, 0.0020, 10),\n",
    "          'l1_ratio': np.linspace(0.1, 0.5, 10)}\n",
    "\"\"\"\n",
    "\n",
    "params = {'alpha': [0.0008], 'l1_ratio': [0.36]}\n",
    "\n",
    "grid = GridSearchCV(en_model, params, cv=5, scoring='neg_mean_squared_error')\n",
    "grid.fit(X, Y)\n",
    "best_params = grid.best_params_\n",
    "best_score = grid.best_score_\n",
    "best_knn = grid.best_estimator_\n",
    "print(best_score)\n",
    "en_coefs = pd.Series(grid.best_estimator_.coef_,index=X.columns)\n",
    "\n",
    "y_pred_en = grid.best_estimator_.predict(X)\n",
    "\n",
    "# Plot residuals\n",
    "residuals_train = y_pred_en - Y\n",
    "data['residual'] = residuals_train\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(y_pred_en, residuals_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and Future Work\n",
    "\n",
    "The results for all methods are summarized in the following table:\n",
    "\n",
    "| Score        | Elastic Net | Ridge    | Lasso    | XGBoost  |\n",
    "|--------------|-------------|----------|----------|----------|\n",
    "| CV RMSE      | -0.00827    | -0.00913 | -0.00863 | -0.0130 |\n",
    "| Kaggle RMSLE | 0.11501     | 0.11548  | 0.11578  | 0.12755  |\n",
    "\n",
    "\n",
    "The Elastic Net CV RMSE is the lowest of all models, and correspondingly, the RMSLE of the kaggle holdout set is minimized. This intuitvely makes sense as Elastic Net combines the L1 and L2 regularization terms found in Lasso and Ridge respectively. In effect Elastic Net is combining the strengths of both models, allowing it to outperform Lasso and Ridge.\n",
    "\n",
    "We see that the CV RMSE for XGBoost is 50% higher than the other methods, and performance on the holdout set is poor. The underrfitting of XGBoost would likely be reduced by further hyperparameter optimization. For the sake of computation time some parameters such as n_estimators, reg_alpha and learning_rate were kept constant. By performing a grid search over these parameters would likely improve this score. In order to tackle the computation time issue, Bayesian Optimization methods could be employed.\n",
    "\n",
    "The biggest improvements in model performance came from feature engineering. The baseline case with no feature engineering scored and RMSLE of ~0.14. With numeric ordinal feature coding, 'LotFrontage' linear imputation, and polynomial terms the RMSLE improved to 0.12. With new feature creation and residual outlier removal a score of 0.18 could be achieved. Finally, with the addition of second order interaction terms the final score of 0.115 was obtained.\n",
    "\n",
    "Further improvements to the model can be achieved via optimization of the number of second-order interaction terms, further hyper-parameter optimization of XGBoost, troubleshooting of the LotFrontage imputation residual plot, and the testing of models not used in this project such as SVM, neural networks and model ensembling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
